#!/usr/bin/python
#
# pbs-spark-submit:  Run an Apache Spark "job" (including optionally
#                    starting the Spark services) inside a PBS job.
# Copyright 2014, 2015 University of Tennessee
# Copyright 2015-2020 Ohio Supercomputer Center
#
# License:  GNU GPL v2; see ../COPYING for details.
# Revision info:
# $HeadURL$
# $Revision$
# $Date$
import argparse
import fcntl
import getopt
import glob
import os
import platform
import socket
import struct
import sys
import time
import warnings

#
# ways to launch workers
#
class Launcher:
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],
               wpn=1,worker_on_mother_superior=True):
        raise NotImplementedError
    def sleep(self):
        sleeptime = 5
        if ( "PBS_NUM_NODES" in os.environ ):
            sleeptime += 2*int(os.environ["PBS_NUM_NODES"])
        elif ( "SLURM_JOB_NUM_NODES" in os.environ ):
            sleeptime += 2*int(os.environ["SLURM_JOB_NUM_NODES"])
        elif ( "SLURM_NNODES" in os.environ ):
            sleeptime += 2*int(os.environ["SLURM_NNODES"])
        time.sleep(sleeptime)
    def env_list(env,prop_env_list):
        # since we can't rely on ssh_config and sshd_config having
        # the appropriate SendEnv/AcceptEnv settings
        argv = []
        for var in prop_env_list:
            if ( var in env.keys() ):
                argv.append(var+"="+env[var])
        return argv
    def env_string(env,prop_env_list):
        return " ".join(self.env_list(env,prop_env_list))


class ExecLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],
               wpn=1,worker_on_mother_superior=True):
        time.sleep(1)

        # sanity check
        if ( not worker_on_mother_superior ):
            raise RuntimeError("Cannot use --no-worker-on-mother-superior with Exec launcher")
        
        # lots of squick to try to limit the number of cores used on big
        # SMP/NUMA systems that are likely shared with other users
        cpuset = None
        cpusetroot = None
        cpus = 0
        if ( os.path.exists("/proc/self/cpuset") ):
            cpusetfile = open("/proc/self/cpuset")
            cpuset = cpusetfile.read().rstrip("\n")
            cpusetfile.close()
        if ( os.path.exists("/dev/cpuset") ):
            cpusetroot = "/dev/cpuset"
        elif ( os.path.exists("/sys/fs/cgroup/cpuset") ):
            cpusetroot = "/sys/fs/cgroup/cpuset"
        if ( cpusetroot is not None and cpuset is not None ):
            cpusfile = None
            if ( os.path.exists(cpusetroot+cpuset+"/cpus") ):
                cpusfile = open(cpusetroot+cpuset+"/cpus")
            elif ( os.path.exists(cpusetroot+cpuset+"/cpuset.cpus") ):
                cpusfile = open(cpusetroot+cpuset+"/cpuset.cpus")
            if ( cpusfile is not None ):
                allcpus = cpusfile.read()
                cpusfile.close()
                for cgroup in allcpus.split(","):
                    cpurange = cgroup.split("-")
                    if ( len(cpurange)==1 ):
                        cpus += 1
                    elif ( len(cpurange)==2 ):
                        cpus += int(cpurange[1])-int(cpurange[0])+1
            if ( cpus==0 and "PBS_NP" in os.environ.keys() ):
                try:
                    cpus = int(os.environ["PBS_NP"])
                except e,Exception:
                    pass
            elif ( cpus==0 and "PBS_NUM_PPN" in os.environ.keys() ):
                try:
                    cpus = int(os.environ["PBS_NUM_PPN"])
                except e,Exception:
                    pass
        if ( cpus>0 ):
            os.environ["SPARK_WORKER_CORES"] = str(cpus)
            env["SPARK_WORKER_CORES"] = str(cpus)
        # need to do the equivalent shenanigans for memory at some point...
        # base functionality
        argv = cmdline.split()
        if ( propagate_env ):
            for arg in self.env_list(env,prop_arg_list):
                argv.append(arg)
        child_pid = os.fork()
        if ( child_pid==0 ):
            os.execvpe(argv[0],argv,env)
        self.sleep()


class PBSDSHLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],
               wpn=1,worker_on_mother_superior=True):
        time.sleep(1)
        cmd = cmdline
        if ( propagate_env ):
            cmd = self.env_string(env,prop_env_list)+" "+cmdline
        if ( wpn is None ):
            os.system("pbsdsh "+cmdline+" &")
        else:
            nodes = nodelist(unique=True)
            for node in nodes:
                if ( worker_on_mother_superior or
                     not ( node in platform.node() ) ):
                    for i in range(int(wpn)):
                        os.system("pbsdsh -h "+node+" "+cmdline+" &")
        self.sleep()

class SRunLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],
               wpn=1,worker_on_mother_superior=True):
        time.sleep(1)
        cmd = cmdline
        if ( propagate_env ):
            cmd = self.env_string(env,prop_env_list)+" "+cmdline
        if ( wpn is None ):
            os.system("srun %s &" % cmd)
        else:
            # srun is a bit dense and has to be told the total number of tasks
            # with --ntasks-per-node
            if ( "SLURM_JOB_NUM_NODES" in os.environ ):
                nnodes = int(os.environ["SLURM_JOB_NUM_NODES"])
            elif ( "SLURM_NNODES" in os.environ ):
                nnodes = int(os.environ["SLURM_NNODES"])
            else:
                raise RuntimeError("SRunLauncher:  Unable to determine number of nodes")
            srun = "srun"
            if ( not worker_on_mother_superior ):
                nnodes -= 1
                srun = "srun --exclude=%s" % platform.node().split(".")[0]
            ntasks = nnodes*wpn
            os.system("%s --nodes=%d --ntasks=%d --ntasks-per-node=%d %s &" % (srun,nnodes,ntasks,wpn,cmd))
        self.sleep()

class SSHLauncher(Launcher):
    def launch(self,cmdline,env,propagate_env=False,
               prop_env_list=["SPARK_CONF_DIR","SPARK_LOG_DIR","SPARK_LOCAL_DIRS"],
               wpn=1,worker_on_mother_superior=True):
        time.sleep(1)
        if ( "PBS_NODEFILE" in os.environ.keys() ):
            if ( wpn is None ):
                nodes = nodelist()
            else:
                nodes = nodelist(unique=True)
            for node in nodes:
                if ( worker_on_mother_superior or
                     not ( node in platform.node() ) ):
                    argv = cmdline.split()
                    ssh = "ssh"
                    if ( "SPARK_SSH" in env.keys() ):
                        ssh=env["SPARK_SSH"]
                    argv.insert(0,ssh)
                    argv.insert(1,node)
                    if ( propagate_env ):
                        for arg in self.env_list(env,prop_env_list):
                            argv.insert(2,arg)
                    sys.stderr.write(" ".join(argv)+"\n")
                    if ( wpn is None ):
                        nforks = 1
                    else:
                        nforks = int(wpn)
                    for i in range(nforks):
                        child_pid = os.fork()
                        if ( child_pid==0 ):
                            os.execvpe(argv[0],argv,env)   
            self.sleep()
        else:
            raise EnvironmentError("PBS_NODEFILE undefined")

#
# functions to help with PBS node file
#
def nodelist(unique=False):
    nodes = []
    if ( "PBS_NODEFILE" in os.environ.keys() ):
        nodefile = open(os.environ["PBS_NODEFILE"])
        for line in nodefile.readlines():
            node = line.rstrip("\n")
            if ( not unique or not ( node in nodes ) ):
                nodes.append(node)
    return nodes


#
# functions to help with handling Java properties
#
def propsToCmdLine(proplist):
    result = []
    for prop in proplist.keys():
        result.append("-D"+prop+"=\""+proplist[prop]+"\"")
    return " ".join(result)

def propsFromFile(filename):
    if ( not os.path.exists(filename) ):
        raise IOError(filename+" not found")
    proplist = {}
    fd = open(filename)
    for line in fd.readlines():
        if ( not line.startswith("#") ):
            keyval = (line.rstrip("\n")).split("=",1)
            if ( len(keyval)==2 ):
                proplist[keyval[0]] = keyval[1]
    return proplist

#
# get IP address of network interface
# borrowed from http://code.activestate.com/recipes/439094-get-the-ip-address-associated-with-a-network-inter/
#
def get_ip_address(ifname):
    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    return socket.inet_ntoa(fcntl.ioctl(
            s.fileno(),
            0x8915,  # SIOCGIFADDR
            struct.pack('256s', ifname[:15])
            )[20:24])

#
# main program begins here
#

# set up default environment
properties = {}
launcher = SSHLauncher()
if ( "SPARK_LAUNCHER" in os.environ.keys() ):
    if ( os.environ["SPARK_LAUNCHER"] in ("exec","EXEC") ):
        launcher = ExecLauncher()
    if ( os.environ["SPARK_LAUNCHER"] in ("pbsdsh","PBSDSH") ):
        launcher = PBSDSHLauncher()
    if ( os.environ["SPARK_LAUNCHER"] in ("ssh","SSH") ):
        launcher = SSHLauncher()
    if ( os.environ["SPARK_LAUNCHER"] in ("srun","SRUN") ):
        launcher = SRunLauncher()
elif ( "SLURM_JOB_ID" in os.environ ):
    launcher = SRunLauncher()
elif ( "PBS_JOBID" in os.environ ):
    launcher = PBSDSHLauncher()
if ( not ( "SPARK_CONF_DIR" in os.environ.keys() ) ):
    os.environ["SPARK_CONF_DIR"] = os.getcwd()+"/conf"
if ( not ( "SPARK_LOG_DIR" in os.environ.keys() ) ):
    os.environ["SPARK_LOG_DIR"] = os.getcwd()

# manage scratch directories
# **ASSUMPTION**:  work directory is on a shared file system
workdir = os.getcwd()
if ( "SCRATCHDIR" in os.environ.keys() ):
    if ( "SLURM_JOB_ID" in os.environ ):
        workdir = os.environ["SCRATCHDIR"]+"/spark-"+os.environ["SLURM_JOB_ID"]
    elif ( "PBS_JOBID" in os.environ ):
        workdir = os.environ["SCRATCHDIR"]+"/spark-"+os.environ["PBS_JOBID"]
# SPARK_LOCAL_DIRS should be node-local
if ( ( "TMPDIR" in os.environ.keys() ) and
     not ( "SPARK_LOCAL_DIRS" in os.environ.keys() ) ):
    os.environ["SPARK_LOCAL_DIRS"] = os.environ["TMPDIR"]
elif ( not ( "SPARK_LOCAL_DIRS" in os.environ.keys() ) ):
    os.environ["SPARK_LOCAL_DIRS"] = "/tmp"

# command line argument handling
parser = argparse.ArgumentParser()
initargs = parser.add_mutually_exclusive_group()
initargs.add_argument("--init",
                      help="Initialize Spark master/worker services (default).",
                      action="store_true",
                      dest="init_svcs",
                      default=True)
initargs.add_argument("--no-init",
                      help="Do not initialize Spark master/worker services.",
                      dest="init_svcs",
                      action="store_false")
launcherargs = parser.add_mutually_exclusive_group()
launcherargs.add_argument("--pbsdsh",
                          help="Use the pbsdsh process launcher.",
                          action="store_const",
                          dest="launcher",
                          const="pbsdsh",
                          default=None)
launcherargs.add_argument("--exec",
                          help="Use the exec process launcher.",
                          action="store_const",
                          dest="launcher",
                          const="exec")
launcherargs.add_argument("--srun",
                          help="Use the srun process launcher.",
                          action="store_const",
                          dest="launcher",
                          const="srun")
launcherargs.add_argument("--ssh",
                          help="Use the ssh process launcher.",
                          action="store_const",
                          dest="launcher",
                          const="ssh")
msargs = parser.add_mutually_exclusive_group()
msargs.add_argument("--worker-on-mother-superior",
                    help="Run a worker on the mother superior node as well as the driver program (default).",
                    action="store_true",
                    dest="worker_on_mother_superior",
                    default=True)
msargs.add_argument("-N","--no-worker-on-mother-superior",
                    help="Do not run a worker on the mother superior node, only the driver program.",
                    action="store_false",
                    dest="worker_on_mother_superior")
parser.add_argument("-M","--master-interface",
                    help="Have Spark master listen on network interface <IF> rather than the default.",
                    metavar="IF",
                    dest="iface",
                    type=str,
                    default=None)
parser.add_argument("-C","--conf-dir",
                    help="Look in <SPARK_CONF_DIR> for Java properties files.",
                    dest="spark_conf_dir",
                    type=str,
                    default=None)
parser.add_argument("-L","--log-dir",
                    help="Place logs in <SPARK_LOG_DIR>.",
                    dest="spark_log_dir",
                    type=str,
                    default=None)
parser.add_argument("-l","--log4j-properties",
                    help="Read log4j properties from <LOG4J_PROPS>.",
                    dest="log4j_props",
                    type=str,
                    default=None)
parser.add_argument("-d","--work-dir",
                    help="Use <workdir> as Spark program's working directory.",
                    metavar="workdir",
                    dest="workdir",
                    type=str,
                    default=workdir)
parser.add_argument("-W","--wpn","--workers-per-node",
                    help="Launch <N> worker tasks per node instead of the default (1).",
                    metavar="N",
                    dest="wpn",
                    type=int,
                    default=1)
parser.add_argument("-w","--wc","--worker-cores",
                    help="Use <N> cores per worker (default all available).",
                    metavar="N",
                    dest="worker_cores",
                    type=int,
                    default=None)
parser.add_argument("-m","--wm","--worker-memory",
                    help="Set per-worker memory limit.",
                    metavar="MEM",
                    dest="worker_mem",
                    type=str,
                    default=None)
parser.add_argument("-p","--pausetime",
                    help="Pause <N> seconds between startup stages (default 5).",
                    metavar="N",
                    type=int,
                    default=5)
parser.add_argument("-D","--conf",
                    help="Set the Java property <key> to <value>.",
                    metavar="key=value",
                    dest="properties",
                    nargs='+',
                    action="append")
parser.add_argument("-P","--properties-file",
                    help="Read Java properties from <propfile>.",
                    metavar="propfile",
                    dest="propfile",
                    type=str,
                    default=None)
parser.add_argument("--class",
                    help="Application's main class (for Java/Scala apps).",
                    dest="classname",
                    type=str,
                    default=None)
parser.add_argument("--name",
                    help="The name of your application.",
                    type=str,
                    default=None)
parser.add_argument("--jars",
                    help="Comma-separated list of local jars to include on the driver and executor classpaths.",
                    metavar="jarlist",
                    type=str,
                    default=None)
parser.add_argument("--packages",
                    help="Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version.",
                    metavar="pkglist",
                    dest="pkgs",
                    type=str,
                    default=None)
parser.add_argument("--exclude-packages",
                    help="Comma-separated list of groupId:artifactId to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts.",
                    metavar="pkglist",
                    dest="excl_pkgs",
                    type=str,
                    default=None)
parser.add_argument("--repositories",
                    help="",
                    metavar="repolist",
                    dest="repos",
                    type=str,
                    default=None)
parser.add_argument("--py-files",
                    help="Comma-separated list of .zip, .egg, or .py files to place on PYTHONPATH for Python apps.",
                    metavar="filelist",
                    dest="pyfiles",
                    type=str,
                    default=None)
parser.add_argument("--files",
                    help="Comma-separated list of files to be placed in the working directory of each executor.",
                    type=str,
                    default=None)
parser.add_argument("--driver-memory",
                    help="Memory for driver (e.g. 1000M, 2G; default is 1024M).",
                    metavar="mem",
                    dest="driver_mem",
                    type=str,
                    default=None)
parser.add_argument("--driver-java-options",
                    help="Extra Java options to pass to the driver.",
                    metavar="opts",
                    dest="driver_java_opts",
                    type=str,
                    default=None)
parser.add_argument("--driver-library-path",
                    help="Extra library path entries to pass to the driver.",
                    metavar="libpath",
                    dest="driver_lib_path",
                    type=str,
                    default=None)
parser.add_argument("--driver-class-path",
                    help="Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath.",
                    metavar="classpath",
                    dest="driver_class_path",
                    type=str,
                    default=None)
parser.add_argument("--executor-cores",
                    help="# cores per executor (e.g. 1; default is all available).",
                    metavar="cores",
                    dest="exec_cores",
                    type=int,
                    default=None)
parser.add_argument("--executor-memory",
                    help="Memory per executor (e.g. 1000M, 2G; default is 1G).",
                    metavar="mem",
                    dest="exec_mem",
                    type=str,
                    default=None)
parser.add_argument("app",
                     nargs=argparse.REMAINDER,
                     help="Spark application and arguments")

args = parser.parse_args()

# post arg parsing setup
if ( args.launcher is not None ):
    if ( args.launcher=="exec" ):
        launcher = ExecLauncher()
    elif ( args.launcher=="pbsdsh" ):
        launcher = PBSDSHLauncher()
    elif ( args.launcher=="srun" ):
        launcher = SRunLauncher()
    elif ( args.launcher=="ssh" ):
        launcher = SSHLauncher()
if ( args.properties is not None ):
    for propertylist in args.properties:
        for property in propertylist:
            try:
                (key,value) = property.split("=")
                properties[key] = value
            except:
                sys.stdin.write("Ignoring malformed property \"%s\"\n" % property)

# environment sanity checks
if ( not "PBS_JOBID" in os.environ and not "SLURM_JOB_ID" in os.environ ):
    raise EnvironmentError("Not in a PBS or Slurm job")
if ( not ( "SPARK_HOME" in os.environ.keys() ) ):
    raise EnvironmentError("SPARK_HOME not defined")

# read any properties files in the conf directory
for propfile in glob.glob(os.environ["SPARK_CONF_DIR"]+"/*.properties"):
    if ( os.path.exists(propfile) ):
        props = propsFromFile(propfile)
        for key in props.keys():
            if ( not ( key in properties.keys() ) ):
                properties[key] = props[key]
print(properties)

# make sure the work dir actually exists
if ( args.workdir is not None and not os.path.exists(args.workdir) ):
    os.mkdir(args.workdir)

# **ASSUMPTION**:  master runs on mother superior node
if ( args.iface is None ):
    os.environ["SPARK_MASTER_IP"] = platform.node()
    os.environ["SPARK_MASTER_HOST"] = platform.node()
else:
    os.environ["SPARK_MASTER_IP"] = get_ip_address(args.iface)
    os.environ["SPARK_MASTER_HOST"] = get_ip_address(args.iface)
if ( not ( "SPARK_MASTER_PORT" in os.environ.keys() ) ):
    os.environ["SPARK_MASTER_PORT"] = "7077"
spark_master = "spark://"+os.environ["SPARK_MASTER_IP"]+":"+str(os.environ["SPARK_MASTER_PORT"])
#sys.stderr.write("Spark master = "+spark_master+"\n")

if ( args.init_svcs ):
    # stick any properties in the appropriate environment variable
    if ( len(properties)>0 ):
        if ( "SPARK_DAEMON_JAVA_OPTS" in os.environ.keys() ):
            os.environ["SPARK_DAEMON_JAVA_OPTS"] += " "+propsToCmdLine(properties)
        else:
            os.environ["SPARK_DAEMON_JAVA_OPTS"] = propsToCmdLine(properties)

    # launch master on mother superior
    cmdline = os.environ["SPARK_HOME"]+"/sbin/start-master.sh"
    os.system(cmdline+" &")
    sys.stderr.write(cmdline+"\n")
    sys.stdout.write("SPARK_MASTER_HOST="+os.environ["SPARK_MASTER_HOST"]+"\n")
    sys.stdout.write("SPARK_MASTER_PORT="+os.environ["SPARK_MASTER_PORT"]+"\n")
    time.sleep(args.pausetime)

    # launch workers
    cmdline = os.environ["SPARK_HOME"]+"/bin/spark-class org.apache.spark.deploy.worker.Worker"
    if ( args.worker_cores is not None ):
        cmdline += " --cores "+str(args.worker_cores)
    if ( args.worker_mem is not None ):
        cmdline += " --memory "+args.worker_mem
    if ( args.workdir is not None ):
        cmdline += " --work-dir "+args.workdir
    cmdline += " "+spark_master
    sys.stderr.write(cmdline+"\n")
    launcher.launch(cmdline,os.environ,wpn=args.wpn,
                    worker_on_mother_superior=args.worker_on_mother_superior)
    time.sleep(args.pausetime)

# run the user's Spark "job", if one is given
if ( len(args.app)>0 ):
    cmdline = os.environ["SPARK_HOME"]+"/bin/spark-submit --master "+spark_master
    if ( args.classname is not None ):
        cmdline += " --class "+args.classname
    if ( args.name is not None ):
        cmdline += " --name "+args.name
    if ( args.jars is not None ):
        cmdline += " --jars "+args.jars
    if ( args.pkgs is not None ):
        cmdline += " --packages "+args.pkgs
    if ( args.excl_pkgs is not None ):
        cmdline += " --exclude-packages "+args.excl_pkgs
    if ( args.repos is not None ):
        cmdline += " --repositories "+args.repos
    if ( args.pyfiles is not None ):
        cmdline += " --py-files "+args.pyfiles
    if ( args.files is not None ):
        cmdline += " --files "+args.files
    if ( args.log4j_props is not None and args.driver_java_opts is None ):
        cmdline += " --driver-java-options \"-Dlog4j.configuration=file:"+args.log4j_props+"\""
    elif ( args.log4j_props is None and args.driver_java_opts is not None ):
        cmdline += " --driver-java-options \""+args.driver_java_opts+"\""
    elif ( args.log4j_props is not None and args.driver_java_opts is not None ):
        cmdline += " --driver-java-options \"-Dlog4j.configuration=file:"+args.log4j_props+" "+args.driver_java_opts+"\""
    if ( args.driver_mem is not None ):
        cmdline += " --driver-memory "+args.driver_mem
    if ( args.driver_lib_path is not None ):
        cmdline += " --driver-library-path "+args.driver_lib_path
    if ( args.driver_class_path is not None ):
        cmdline += " --driver-class-path "+args.driver_class_path
    if ( args.exec_cores is not None ):
        cmdline += " --executor-cores "+args.exec_cores
    if ( args.exec_mem is not None ):
        cmdline += " --executor-memory "+args.exec_mem
    for key in properties.keys():
        cmdline += " --conf \""+str(key)+"="+str(properties[key])+"\""
    cmdline += " "+" ".join(args.app)
    print(cmdline)
    os.system(cmdline)